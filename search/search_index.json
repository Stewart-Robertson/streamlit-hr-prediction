{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Business Turnover &amp; Savings Estimator","text":"<p>What this is: An interactive app that predicts which employees are at risk of leaving and estimates how much money you could save by taking targeted retention actions.</p> <p>Who it\u2019s for: HR leaders, People Partners, Finance/C-suite.</p> <p>What you can do here - See risk across the organisation at a glance - Try what-if scenarios (e.g., lighten workload, improve manager quality) - Choose an intervention strategy (Threshold or Top-K) - Quantify expected savings and prioritise where to act</p> <p>\ud83d\udc49 Start with How to use (2\u20133 minutes).</p> <p>\ud83d\udd17 Connect with me: LinkedIn</p>"},{"location":"01_source_data_dictionary/","title":"Employee Attrition Dataset - Data Dictionary","text":"<p>A comprehensive data dictionary covering all columns in the employee attrition prediction dataset, including derived fields and embedded signals for modeling.</p>"},{"location":"01_source_data_dictionary/#identifiers-dates","title":"\ud83d\udccb Identifiers &amp; Dates","text":"Column Type Values / Range Description Embedded Signal / Notes <code>employee_id</code> Integer Unique Employee identifier Do not use in modelling (identifier only). <code>snapshot_date</code> Date (YYYY-MM-DD) 2020-01-01 \u2192 2024-12-31 Observation date for features Use for time-aware splits (e.g., train \u22642023, test = 2024). <code>hire_date</code> Date (YYYY-MM-DD) Earlier than snapshot_date Original hire date Used to derive tenure. <code>tenure_years</code> Float (2 d.p.) \u2265 0 Years at company at snapshot U-shape risk: higher in first year, dip in years 1\u20133, slight rise after ~7y. <code>months_since_hire</code> Float (1 d.p.) \u2265 0 Months since hire at snapshot Same signal as tenure; convenience for models."},{"location":"01_source_data_dictionary/#organization-role","title":"\ud83c\udfe2 Organization &amp; Role","text":"Column Type Values / Range Description Embedded Signal / Notes <code>region</code> Categorical AMER, EMEA, APAC Work region Mild mix effects only (no strong bias). <code>department</code> Categorical Sales, Engineering, Customer Success, Marketing, Finance, HR, Operations, Product Functional area Small base-rate shifts by department (e.g., Sales slightly higher). <code>role</code> Categorical Dept-specific titles (e.g., SWE II, PM, AE, CSM, etc.) Job title Captures labour-market pull and career path nuance. <code>level</code> Categorical IC1, IC2, IC3, IC4, Manager, Director, VP Seniority level Drives market pay reference; leadership relates to lower attrition. <code>is_manager</code> Binary (0/1) 0 or 1 Manager indicator Managers tend to attrit less, all else equal. <code>team_id</code> Integer 1\u2013400 Team cluster ID Adds random effects \u2192 correlated outcomes within teams."},{"location":"01_source_data_dictionary/#compensation-progression","title":"\ud83d\udcb0 Compensation &amp; Progression","text":"Column Type Values / Range Description Embedded Signal / Notes <code>base_salary</code> Float (0 d.p.) \u2248 45k\u2013\u221e (currency-agnostic) Annual base pay Relative to market reference matters more than absolute. <code>compa_ratio</code> Float (2 d.p.) \u2248 0.6\u20131.5 (typ.) base_salary / market_ref(dept\u00d7level) Below 1.0 increases risk; above 1.0 reduces risk. <code>avg_raise_3y</code> Float (3 d.p.) \u2265 0 (typ. ~0\u20130.08) Mean merit raise % over last 3 cycles Higher raises \u2192 lower risk. <code>time_since_last_promo_yrs</code> Float (2 d.p.) \u2265 0 Years since last promotion Risk rises after ~3 years without promotion. <code>internal_moves_last_2y</code> Integer 0+ Internal transfers in last 2 years More internal mobility \u2192 lower risk. <code>stock_grants</code> Float (0 d.p.) 0 or &gt;0 for senior/IC4+ Equity value (approx.) Higher equity slightly reduces risk. <code>salary_band</code> Categorical Q1\u2026Q5 Pay quantile across the snapshot Convenience feature; derived from base_salary."},{"location":"01_source_data_dictionary/#performance-engagement-development","title":"\ud83d\udcc8 Performance, Engagement &amp; Development","text":"Column Type Values / Range Description Embedded Signal / Notes <code>performance_rating</code> Integer 1\u20135 Most recent performance score Higher rating \u2192 lower risk (also interacts with progression). <code>engagement_score</code> Float (2 d.p.) 1\u201310 or NaN Survey engagement Lower \u2192 higher risk. MNAR missingness (disengaged more likely missing). <code>manager_quality</code> Float (2 d.p.) 1\u201310 or NaN Perceived manager quality Lower \u2192 higher risk. MNAR missingness (worse mgr \u2192 more missing). <code>workload_score</code> Float (2 d.p.) 1\u201310 Self-reported workload Higher workload increases risk, especially if manager quality is low (interaction). <code>learning_hours_last_yr</code> Float (1 d.p.) \u2265 0 Formal learning hours last year Slight retention effect (more learning \u2192 slightly lower risk). <code>benefit_score</code> Float (2 d.p.) 1\u201310 Perceived benefits quality Higher benefits \u2192 lower risk."},{"location":"01_source_data_dictionary/#work-pattern-schedule-commute","title":"\ud83c\udfe0 Work Pattern, Schedule &amp; Commute","text":"Column Type Values / Range Description Embedded Signal / Notes <code>remote_status</code> Categorical Remote, Hybrid, Onsite Work arrangement Onsite/Hybrid carry small added risk vs Remote. <code>commute_km</code> Float (1 d.p.) 0\u2013high One-way commute distance Longer commute \u2192 higher risk, strongest for Onsite (interaction). <code>overtime_hours_month</code> Float (1 d.p.) \u2265 0 Avg monthly overtime Heavier OT correlates with risk via workload sentiment. <code>night_shift</code> Binary (0/1) 0 or 1 Night-shift indicator Increases risk. <code>schedule_flex</code> Binary (0/1) 0 or 1 Flexible schedule option Reduces risk."},{"location":"01_source_data_dictionary/#time-off-leaves","title":"\ud83c\udfd6\ufe0f Time Off &amp; Leaves","text":"Column Type Values / Range Description Embedded Signal / Notes <code>sick_days</code> Float (1 d.p.) \u2265 0 Sick days taken in last year Small non-linear signal (both unusually high/low carry info). <code>pto_days_taken</code> Float (1 d.p.) 0\u201340 (typ.) Paid time off taken in last year Contextual; moderate PTO often neutral/healthy. <code>leave_last_yr</code> Binary (0/1) 0 or 1 Any extended leave last year Slight positive risk bump."},{"location":"01_source_data_dictionary/#target-known-leakage-columns","title":"\ud83c\udfaf Target &amp; Known Leakage Columns","text":"Column Type Values / Range Description Embedded Signal / Notes <code>attrited</code> Binary (0/1) 0 or 1 Target: left in the next period Drawn from structured probability combining all signals (incl. team &amp; macro drift). <code>exit_interview_scheduled</code> Binary (0/1) 0 or 1 Process flag \u26a0\ufe0f LEAKAGE: strongly tied to outcome; exclude at train time. <code>offboarding_ticket_created</code> Binary (0/1) 0 or 1 IT/HR offboarding flag \u26a0\ufe0f LEAKAGE: strongly tied to outcome; exclude at train time."},{"location":"case_study/","title":"Fictional Case Study: Reducing Avoidable Turnover at XY Financial","text":""},{"location":"case_study/#summary","title":"Summary","text":"<p>Key Result</p> <p>With 25,000 employees at 17% attrition, XY Financial turned a costly annual problem into a data-backed, quarterly savings plan worth $5\u20137M per quarter, using transparent risk scoring, horizon-scaled economics, and scenario testing.</p>"},{"location":"case_study/#company-profile","title":"Company Profile","text":"Attribute Details Company XY Financial (fictional, based on real project scale) Industry Financial Services Employees 25,000 Baseline Attrition ~17% annually (\u22484,250 exits/year) Goal Reduce costly turnover and evaluate ROI of proactive retention programs under financial constraints"},{"location":"case_study/#1-business-problem","title":"1. Business Problem","text":"<p>XY was losing ~4,250 employees per year, straining recruitment and productivity. Replacing each leaver cost well over a year's salary when accounting for:</p> <ul> <li>Hiring costs</li> <li>Onboarding expenses  </li> <li>Lost productivity</li> </ul> <p>HR wanted to pilot targeted interventions (e.g., manager coaching, workload adjustments, retention bonuses), but Finance insisted on a rigorous, data-backed business case before releasing budget.</p>"},{"location":"case_study/#2-approach-using-the-predictor-app","title":"2. Approach Using the Predictor App","text":"<p>We used the Attrition Savings Predictor to:</p> <ol> <li>Estimate risk of departure for every employee (25k rows scored)</li> <li>Compare intervention strategies: fixed risk thresholds vs Top-K targeting</li> <li>Apply horizon scaling (quarterly vs annual) so CFOs saw costs and savings on a realistic budget lens</li> <li>Run what-if scenarios (reducing workload, improving manager quality) to simulate program levers</li> <li>Locate the \"economically optimal threshold\" \u2014 where expected savings outweigh costs the most</li> </ol>"},{"location":"case_study/#3-assumptions-parameters","title":"3. Assumptions &amp; Parameters","text":""},{"location":"case_study/#financial-parameters","title":"Financial Parameters","text":"<ul> <li>Replacement cost: 1.2\u00d7 annual salary (scaled to horizon)</li> <li>Average salary: $82,000</li> <li>Effective replacement cost (annual view): ~$98,400</li> <li>Intervention cost: $750 per participant (lump sum)</li> <li>Effectiveness: 30\u201335% (probability that a likely leaver stays if treated)</li> <li>Planning horizon: tested annual vs quarterly</li> </ul>"},{"location":"case_study/#model-performance","title":"Model Performance","text":"Metric Value Interpretation AUC (ROC) ~0.68 Good ranking ability Precision @ Top 10% ~0.41 4 out of 10 flagged will actually leave Lift (Top 10% vs baseline) ~2.4\u00d7 Top 10% risk group 2.4\u00d7 more likely to leave Calibration \u00b13 points Predicted risk matches observed outcomes"},{"location":"case_study/#4-strategy-experiments","title":"4. Strategy Experiments","text":""},{"location":"case_study/#a-threshold-based-plan","title":"A) Threshold-Based Plan","text":"<ul> <li>Threshold: ~0.28</li> <li>Employees flagged: ~6,000</li> <li>Expected true leavers: ~1,400</li> </ul>"},{"location":"case_study/#savings-annual-view","title":"Savings (Annual View)","text":"Component Calculation Amount Gross prevented attrition 1,400 \u00d7 0.35 \u00d7 $98,400 $48M Intervention cost 6,000 \u00d7 $750 $4.5M Net savings Gross - Cost $43.5M"},{"location":"case_study/#b-top-k-strategy-top-15-coverage-60","title":"B) Top-K Strategy (Top 15%, Coverage 60%)","text":"<ul> <li>Cohort: 25,000 \u00d7 15% \u00d7 60% \u2248 2,250 treated employees</li> <li>Characteristics: Riskier group than threshold baseline; higher precision</li> <li>Net savings (annual view): ~$21M</li> <li>Quarterly horizon: drops to ~$5.2M, but still positive ROI</li> </ul>"},{"location":"case_study/#c-what-if-leverage","title":"C) What-If Leverage","text":"<p>Applied improvements to treated cohort: - Workload reduction: -1.0 - Manager quality improvement: +0.5</p>"},{"location":"case_study/#results","title":"Results","text":"<ul> <li>Aggregate risk delta: ~24 points reduction</li> <li>\u0394 Net savings: +$6M annually vs baseline plan</li> <li>Key insight: Gain achieved without extra per-head program cost \u2014 from shifting predictors the model recognized as causal</li> </ul>"},{"location":"case_study/#5-outcome-decision","title":"5. Outcome &amp; Decision","text":""},{"location":"case_study/#executive-decision","title":"Executive Decision","text":"<ul> <li>Chose: Top-K pilot (15% \u00d7 60% coverage) in two divisions</li> <li>Horizon: 3-month scaling for budget-relevant costs and benefits</li> <li>Finance approval: Based on app-demonstrated ROI</li> </ul>"},{"location":"case_study/#financial-justification","title":"Financial Justification","text":"Scenario Quarterly Savings Baseline ROI ~$5.2M With What-If Applied ~$7M"},{"location":"case_study/#implementation","title":"Implementation","text":"<ul> <li>HR built an A/B test into the pilot to validate the assumed 30\u201335% effectiveness</li> </ul>"},{"location":"case_study/#6-key-learnings","title":"6. Key Learnings","text":""},{"location":"case_study/#scale-impact","title":"Scale Impact","text":"<p>Scale is everything. At 25k headcount, even modest effect sizes create multi-million dollar swings.</p>"},{"location":"case_study/#transparency-value","title":"Transparency Value","text":"<p>Transparency matters. CFOs trusted the threshold breakeven formula and auditors valued the calibrated logistic regression.</p>"},{"location":"case_study/#financial-alignment","title":"Financial Alignment","text":"<p>Horizon scaling bridged HR and Finance. Annual savings looked huge, but quarterly scaling made it realistic for cashflow planning.</p>"},{"location":"case_study/#targeting-efficiency","title":"Targeting Efficiency","text":"<p>Targeted interventions pay. Top-K + Coverage gave tighter ROI than a blunt threshold.  </p> <p>Note</p> <p>This case study is entirely fictional and \"XY Financial\" does not exist to my knowledge.</p>"},{"location":"faq/","title":"FAQ","text":"<p>Is the model \u201cdeciding\u201d who to keep? No. It\u2019s decision support. Managers and HR choose actions.</p> <p>Why do savings change when I switch to Top-K? Because you\u2019re treating a different cohort (highest-risk first), which often increases ROI.</p> <p>Why does \u0394 Net savings show 0 sometimes? Your scenario didn\u2019t move many people across the threshold. See the threshold-free number for a more sensitive comparison.</p>"},{"location":"glossary/","title":"Glossary","text":"<p>Attrition risk \u2013 probability a person will leave. Effectiveness \u2013 chance an intervention prevents an otherwise likely leaver. Replacement cost \u2013 money lost when someone leaves (recruitment, onboarding, productivity). Top-K \u2013 the highest-risk K% of employees. Coverage \u2013 % of the targeted cohort you can treat. Threshold-free savings \u2013 probability-weighted expected value across the treated group. TP \u2013 True Positives. FP \u2013 False Positives. TN \u2013 True Negatives. FN \u2013 False Negatives. Precision \u2013 TP / (TP + FP) i.e., how often the model correctly identifies a leaver. Recall \u2013 TP / (TP + FN) i.e., the proportion of all leavers correctly identified.  </p>"},{"location":"performance/","title":"Model performance","text":""},{"location":"performance/#roc-curve","title":"ROC curve","text":"<ul> <li>Shows how well the model separates leavers vs stayers over all cut-offs.</li> <li>AUC close to 1.0 is great; ~0.65\u20130.75 is common for HR attrition.</li> <li>Use it to pick reasonable thresholds, not to claim perfection.</li> </ul>"},{"location":"performance/#lift-by-decile","title":"Lift by decile","text":"<ul> <li>Ranks employees by risk, splits into 10 equal buckets.</li> <li>Lift &gt; 2 in the top decile means the model is &gt;2x more likely to flag a leaver than random \u2192 excellent for targeting.</li> <li>This model is approximately 3x more likely to flag a leaver than random in the top decile.</li> </ul>"},{"location":"savings/","title":"Savings explained","text":"<p>We show savings in two complementary ways.</p>"},{"location":"savings/#1-threshold-based-net-savings","title":"1) Threshold-based net savings","text":"<p>What it is: We pick a risk threshold and treat everyone above it.</p> <p>Formula (conceptual): </p> <p>Net = (True Positives \u00d7 effectiveness \u00d7 replacement_cost) - (Treatments \u00d7 intervention_cost)</p> <ul> <li>True Positives: flagged people who would have left</li> <li>Treatments: everyone flagged (TP + FP)</li> </ul> <p>Use it for: operational reporting (how many flagged, precision/recall).</p>"},{"location":"savings/#2-threshold-free-savings-expected-value","title":"2) Threshold-free savings (expected value)","text":"<p>What it is: We sum expected value per person over the treated cohort (Top-K or Threshold, with Coverage), without relying on a single cut-off.</p> <p>Per person Expected Value: effectiveness \u00d7 replacement_cost \u00d7 predicted_risk \u2212 intervention_cost</p> <p>Use it for: comparing scenarios and strategy settings; it reflects how risk actually shifts.</p>"},{"location":"savings/#reading-the-numbers-example","title":"Reading the numbers (example)","text":"Metric Value Net savings (base) $60,217,234 Net savings (scenario) $60,217,234 \u0394 Net savings $0 Threshold-free savings (coverage-adjusted) \u2212$3,421,600 <p>What this means:</p> <p>As work conditions improve, people's attrition risk decreases, so at some mixture of settings these people will cross the threshold and impact the net savings.</p> <ul> <li>The threshold-based result: Net savings (scenario) vs Net Savings (base) didn\u2019t change because your what-if didn\u2019t move enough people across the chosen threshold (so flagged counts and TP/FP stayed the same). </li> <li>The threshold-free result is negative, meaning the scenario made the treated cohort less profitable in expectation (e.g., you spent money treating lower-risk people or your effectiveness/cost assumptions make the lever uneconomical).</li> </ul> <p>What to try next: - Increase the lever strength (e.g., larger workload reduction) - Target smarter (Top-K with moderate Coverage) - Recheck assumptions: effectiveness \u2191 or cost \u2193 often flips EV positive</p>"},{"location":"scenarios/","title":"What-if scenarios &amp; strategies","text":""},{"location":"scenarios/#levers","title":"Levers","text":"<ul> <li>Workload: lower scores reduce risk most for overworked teams</li> <li>Manager quality: improvements matter most where baseline is weak</li> </ul>"},{"location":"scenarios/#strategies","title":"Strategies","text":"<ul> <li>Threshold: aligns with policy (\u201cintervene above X% risk\u201d)</li> <li>Top-K: fixes capacity (\u201cwe can treat ~15% of employees\u201d)</li> <li>Coverage: simulate partial rollout to the chosen cohort</li> </ul> <p>Tip</p> <p>Try Top-K first. It naturally targets the highest ROI group. Refer to the lift chart in How to use</p>"},{"location":"underlying_model/","title":"The Prediction Model","text":"<p>TL;DR</p> <p>The app uses a calibrated logistic regression model to estimate each employee\u2019s probability of leaving. It\u2019s trained on historical snapshots up to 2022 and validated on 2023 to mimic how it would perform in the next year. Results are turned into actionable savings estimates using your assumptions (intervention cost, effectiveness, replacement cost).</p>"},{"location":"underlying_model/#what-the-model-does","title":"What the model does","text":"<p>Think of the model like a weather forecast for attrition:</p> <ul> <li>Each person gets a risk score (0\u2013100%) \u2014 \"how likely is rain (attrition) for this person?\"</li> <li>We don't claim certainty for any individual. Instead, we use these probabilities to prioritise attention (like taking an umbrella if rain is likely)</li> <li>The app lets you test what-if levers (e.g., reduce workload) and instantly see how the forecast and savings might change</li> </ul>"},{"location":"underlying_model/#analogy","title":"Analogy","text":"<p>You could think of logistic regression as a weighted checklist. For example, heavier workload and long commute may push risk up; higher pay competitiveness may push risk down. The model learns how much each factor matters based on past outcomes.</p>"},{"location":"underlying_model/#what-the-model-does-not-do","title":"What the Model Does NOT Do","text":"Limitation Description Decision Making It doesn't decide who to keep or let go \u2014 it's decision support Future Prediction It doesn't know the future. It extrapolates from patterns in your historical snapshots Post-Decision Learning It doesn't see post-decision outcomes (e.g., exit interviews, tickets) that would leak future information into training"},{"location":"underlying_model/#key-design-choices","title":"Key Design Choices","text":""},{"location":"underlying_model/#transparency-by-design","title":"Transparency by Design","text":"<p>Logistic regression is easy to inspect and explain, a better fit for HR decision-making than black-box models when accuracy is similar.</p>"},{"location":"underlying_model/#calibrated-probabilities","title":"Calibrated Probabilities","text":"<p>We calibrate the model so \"30% risk\" roughly means \"30 out of 100 similar employees leave.\"</p>"},{"location":"underlying_model/#time-aware-validation","title":"Time-Aware Validation","text":"<p>Train on \u22642022, evaluate on 2023. This prevents \"peeking\" into the future and gives a realistic performance estimate.</p>"},{"location":"underlying_model/#scenario-ready","title":"Scenario-Ready","text":"<p>We engineered a feature (<code>mgmt_workload_score</code>) so what-if changes to workload and manager quality flow through to risk.</p>"},{"location":"underlying_model/#model-inputs-features","title":"Model Inputs (Features)","text":"<p>From the cleaned dataset (<code>data/processed/hr_attrition_clean.csv</code>), the model uses:</p>"},{"location":"underlying_model/#job-pay","title":"Job &amp; Pay","text":"<ul> <li><code>base_salary</code> (pay level)</li> <li><code>salary_band</code> (dropped to avoid duplication)</li> <li><code>compa_ratio</code> (dropped in final model due to collinearity)</li> </ul>"},{"location":"underlying_model/#performance-progression","title":"Performance &amp; Progression","text":"<ul> <li><code>performance_rating</code></li> <li><code>avg_raise_3y</code> (dropped)</li> <li><code>internal_moves_last_2y</code> (dropped)</li> <li><code>time_since_last_promo_yrs</code></li> </ul>"},{"location":"underlying_model/#work-patterns-wellbeing","title":"Work Patterns &amp; Wellbeing","text":"<ul> <li><code>workload_score</code></li> <li><code>overtime_hours_month</code></li> <li><code>sick_days</code></li> <li><code>pto_days_taken</code></li> </ul>"},{"location":"underlying_model/#engagement","title":"Engagement","text":"<ul> <li><code>engagement_score</code></li> <li><code>manager_quality</code> (missingness is informative; also used in interaction below)</li> </ul>"},{"location":"underlying_model/#logistics","title":"Logistics","text":"<ul> <li><code>commute_km</code></li> <li><code>onsite/remote</code> (if present)</li> <li><code>night_shift</code></li> </ul>"},{"location":"underlying_model/#organizational-context","title":"Organizational Context","text":"<ul> <li><code>department</code></li> <li><code>role</code></li> <li><code>team_id</code> (as categorical signals)</li> </ul>"},{"location":"underlying_model/#engineered-features","title":"Engineered Features","text":"<ul> <li><code>mgmt_workload_score = (10 \u2212 manager_quality) \u00d7 workload_score</code> Captures that high workload under poorer management is especially risky</li> </ul>"},{"location":"underlying_model/#explicitly-excluded-features","title":"Explicitly Excluded Features","text":"Category Features Reason Identifiers <code>employee_id</code> Not predictive Post-Decision Signals <code>exit_interview_scheduled</code>, <code>offboarding_ticket_created</code> Data leakage Duplicates/Collinear <code>salary_band</code>, high-VIF pay proxies (<code>compa_ratio</code>, <code>avg_raise_3y</code>, <code>benefit_score</code>) Redundancy/collinearity Split Key <code>snapshot_year</code> Used to split, not to predict <p>Missing Values</p> <p>Missing <code>manager_quality</code> can itself be a signal. The pipeline treats missing values via encoding/scaling, and the engineered interaction uses a conservative floor (e.g., treats missing as 1 for the \"quality\" term in the what-if mechanism).</p>"},{"location":"underlying_model/#technical-pipeline","title":"Technical Pipeline","text":""},{"location":"underlying_model/#1-preprocessing","title":"1. Preprocessing","text":"<ul> <li>Numerical: <code>StandardScaler(with_mean=False)</code></li> <li>Categorical: <code>OneHotEncoder(handle_unknown=\"ignore\", sparse=True)</code></li> <li>Combined: <code>ColumnTransformer</code></li> </ul>"},{"location":"underlying_model/#2-estimator","title":"2. Estimator","text":"<ul> <li><code>LogisticRegression(max_iter=1000)</code> trained on data \u22642022</li> </ul>"},{"location":"underlying_model/#3-calibration","title":"3. Calibration","text":"<ul> <li><code>CalibratedClassifierCV(cv=\"prefit\")</code> with isotonic or sigmoid chosen by lower Brier score on the 2023 set</li> </ul>"},{"location":"underlying_model/#4-persistence","title":"4. Persistence","text":"<ul> <li>Model: <code>models/attrition_lr_calibrated_train_to_2022_skl171.pkl</code></li> <li>Metrics: <code>models/attrition_lr_calibrated_metrics_train_to_2022_skl171.json</code></li> </ul> <p>Why Calibration?</p> <p>Uncalibrated models can be over- or under-confident. Calibration aligns predicted probabilities with observed rates \u2014 crucial when you convert probabilities into expected savings.</p>"},{"location":"underlying_model/#validation-performance","title":"Validation &amp; Performance","text":""},{"location":"underlying_model/#data-split","title":"Data Split","text":"<ul> <li>Train: Snapshots \u22642022</li> <li>Calibrate/Evaluate: 2023</li> </ul>"},{"location":"underlying_model/#key-metrics","title":"Key Metrics","text":"Metric Purpose ROC AUC Overall ranking quality (closer to 1 is better) PR AUC Useful when attrition is rare Lift by Decile Business-friendly: top 10% risk vs average rate Calibration Through the chosen calibration method (implicit) <p>Interpreting Lift</p> <p>If Decile 1 shows 3\u00d7 lift, your top-risk 10% is three times as likely to contain real leavers as a random 10%. That's strong targeting signal.</p>"},{"location":"underlying_model/#from-probability-to-business-value","title":"From Probability to Business Value","text":"<p>Two complementary savings views power the app:</p>"},{"location":"underlying_model/#1-threshold-based-net-savings","title":"1. Threshold-Based Net Savings","text":"<p>Pick a risk threshold; treat everyone above it.</p> <pre><code>Saved cost = (True Positives \u00d7 effectiveness \u00d7 replacement_cost)\nSpend = (Flagged \u00d7 intervention_cost)\nNet = Saved \u2212 Spend\n</code></pre> <p>Good for: Operational reporting, precision/recall trade-offs</p>"},{"location":"underlying_model/#2-threshold-free-expected-value","title":"2. Threshold-Free Expected Value","text":"<p>Sum expected value across the treated cohort (Top-K or Threshold with Coverage):</p> <pre><code>Per person EV = effectiveness \u00d7 replacement_cost \u00d7 predicted_risk \u2212 intervention_cost\n</code></pre> <p>Good for: Comparing levers &amp; strategies without being sensitive to a single threshold</p>"},{"location":"underlying_model/#why-not-a-complex-model","title":"Why Not a Complex Model?","text":"<p>We tried Random Forest and XGBoost variants; in this dataset, logistic regression performed comparably (AUC around ~0.65) but offered much better interpretability and governance. When accuracy differences are marginal, simpler + explainable is the right HR choice.</p>"},{"location":"underlying_model/#limits-caveats","title":"Limits &amp; Caveats","text":""},{"location":"underlying_model/#causality","title":"Causality","text":"<p>The model captures associations, not guaranteed causes. Use it to prioritise conversations and support, not as an automated decision engine.</p>"},{"location":"underlying_model/#data-drift","title":"Data Drift","text":"<p>If your org changes (hybrid policies, comp structure), refresh training and re-calibrate. The app's time-aware split is a safeguard, not a guarantee.</p>"},{"location":"underlying_model/#group-effects","title":"Group Effects","text":"<p><code>team_id</code> can encode manager/team culture. If governance requires, add GroupKFold validation by team to quantify sensitivity.</p>"},{"location":"underlying_model/#fairness","title":"Fairness","text":"<p>Always review risk &amp; intervention rates by relevant groups (e.g., function, location). Add governance checks before production use.</p>"},{"location":"underlying_model/#model-interpretation","title":"Model Interpretation","text":""},{"location":"underlying_model/#coefficients","title":"Coefficients","text":"<p>In logistic regression, each feature has a weight: - Positive weight \u2192 increases log-odds of attrition - Negative weight \u2192 decreases log-odds of attrition</p> <p>You can export coefficients and a per-feature report for HR/legal review.</p>"},{"location":"underlying_model/#partial-effects","title":"Partial Effects","text":"<p>Use decile tables or partial dependence for top drivers (e.g., workload \u2191, manager quality \u2193).</p>"},{"location":"underlying_model/#calibration-check","title":"Calibration Check","text":"<p>Compare predicted vs actual rate in bins (the app's lift + AUC and calibration choice are proxies; you can add a reliability curve if needed).</p>"},{"location":"underlying_model/#reproducibility-versioning","title":"Reproducibility &amp; Versioning","text":"Component Location Training Code <code>src/train_calibrated_lr.py</code> Serving Code <code>app/main.py</code> Data <code>data/processed/hr_attrition_clean.csv</code> Environment <code>requirements.txt</code> and <code>runtime.txt</code> (Python 3.11) Artifacts Model <code>.pkl</code> and metrics <code>.json</code> versioned by train cutoff year and sklearn tag <p>Enterprise Tip</p> <p>For enterprise, log training runs (data hash, params, metrics) in MLflow or DVC, and schedule periodic re-calibration.</p>"},{"location":"underlying_model/#governance-checklist","title":"Governance Checklist","text":"<p>Ready for productionization:</p> <ul> <li>Data lineage documented (source, refresh cadence, snapshot definition)</li> <li>Feature list reviewed (no prohibited attributes; leakage removed)</li> <li>Validation includes time-based and, if required, grouped CV</li> <li>Calibration validated on out-of-time data; reliability plot archived</li> <li>Fairness report (rates by segment, false positive/negative parity where applicable)</li> <li>Monitoring plan (drift, performance drop alerts)</li> <li>Human-in-the-loop SOP (how HR acts on risk, audit trail)</li> </ul>"},{"location":"underlying_model/#frequently-asked-questions","title":"Frequently Asked Questions","text":""},{"location":"underlying_model/#is-65-auc-good-enough","title":"Is 65% AUC \"good enough\"?","text":"<p>For HR attrition with limited features, ~0.6\u20130.7 is common. Value comes from targeting and what-if planning, not perfect prediction.</p>"},{"location":"underlying_model/#why-does-net-savings-sometimes-show-0-while-threshold-free-savings-changes","title":"Why does \u0394 Net Savings sometimes show 0 while threshold-free savings changes?","text":"<p>Your scenario may shift probabilities without moving many people across the chosen threshold. The threshold-free view captures that subtle improvement.</p>"},{"location":"underlying_model/#can-we-add-more-drivers","title":"Can we add more drivers?","text":"<p>Yes \u2014 especially manager signals, work pattern telemetry, career progression, and comp competitiveness vs market (with governance).</p>"},{"location":"usage/","title":"How to use","text":"<p>Threshold and Top-K</p> <p>The threshold is the lever you use to decide how wide the safety net is. The total savings number shows the trade-off: at lower thresholds, you\u2019re spending more but preventing more turnover losses; at higher thresholds, you spend less but miss savings opportunities. The economically optimal threshold \u2014 the point where this trade-off yields the maximum net benefit \u2014 is key to maximizing your return. Top-K is an alternative to the threshold that treats the highest-risk K% of employees. It\u2019s often more profitable than a fixed threshold as it targets the highest-risk people, but it can be less transparent.</p>"},{"location":"usage/#1-set-assumptions-left-sidebar","title":"1) Set assumptions (left sidebar)","text":"<ul> <li>Intervention cost \u2013 average cost per person for your retention program</li> <li>Effectiveness \u2013 chance the program prevents an otherwise likely leaver</li> <li>Replacement cost \u2013 cost to replace a leaver (recruitment + onboarding + lost productivity)</li> </ul> <p>Breakeven</p> <p>A person becomes economically worth treating when <code>risk \u2265 cost / (effectiveness \u00d7 replacement_cost)</code>.</p>"},{"location":"usage/#2-pick-an-intervention-strategy","title":"2) Pick an intervention strategy","text":"<ul> <li>Threshold: treat everyone above a chosen risk.  </li> <li>Top-K: treat the highest-risk K% (e.g., top 15%).  </li> <li>Coverage %: the portion of that group you can actually reach.  </li> </ul> <p>Note: The economically optimal threshold will often maximize net benefit, but you may adjust the threshold or Top-K percentage for practicality, budget constraints, or operational capacity.</p>"},{"location":"usage/#3-explore-what-if-levers","title":"3) Explore what-if levers","text":"<ul> <li>Reduce workload score \u2013 simulate capacity relief  </li> <li>Improve manager quality \u2013 simulate coaching/leadership support</li> </ul> <p>The charts update instantly. Use the \u0394 Net savings and Risk change panels to judge ROI.</p>"},{"location":"usage/#4-interpret-the-results","title":"4) Interpret the results","text":"<ul> <li>Net savings (base) \u2013 expected savings with current settings before applying what-ifs.  </li> <li>Net savings (scenario) \u2013 expected savings after applying what-ifs.  </li> <li>\u0394 Net savings \u2013 difference between scenario and base (positive = better).  </li> <li>Threshold-free savings \u2013 reflects the probability-weighted expected value across the selected treated group (whether defined by Top-K or Threshold), providing a more comprehensive view beyond a single cut-off.</li> </ul> <p>Tip: Hover over metrics in the app to see plain-language one-liner explanations for each.</p>"},{"location":"usage/#5-decide-act","title":"5) Decide &amp; act","text":"<ul> <li>Start with Top-K targeting and a modest Coverage to pilot based on capacity.  </li> <li>Download the top-risk list (Individuals tab), brief managers, schedule the program.  </li> </ul> <p>Remember: The documentation and explanatory tooltips are designed to support your decision-making process and help communicate insights clearly with stakeholders.</p>"}]}